{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e59a9ae",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 循环神经网络\n",
    "\n",
    "\n",
    "## 计算\n",
    "但在下面我们使用一个简单的代码来说明一下。\n",
    "首先，我们定义矩阵`X`、`W_xh`、`H`和`W_hh`，它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。\n",
    "分别将`X`乘以`W_xh`，将`H`乘以`W_hh`，然后将这两个乘法相加，我们得到一个形状为$(3，4)$的矩阵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1020ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:47:33.707501Z",
     "iopub.status.busy": "2022-12-07T16:47:33.706949Z",
     "iopub.status.idle": "2022-12-07T16:47:36.340283Z",
     "shell.execute_reply": "2022-12-07T16:47:36.339349Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-09-01T06:01:18.436954Z",
     "start_time": "2023-09-01T06:01:15.260869Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b981b7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:47:36.347170Z",
     "iopub.status.busy": "2022-12-07T16:47:36.346704Z",
     "iopub.status.idle": "2022-12-07T16:47:36.383092Z",
     "shell.execute_reply": "2022-12-07T16:47:36.382030Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-09-01T06:01:26.623356Z",
     "start_time": "2023-09-01T06:01:26.585088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.4151, -0.3658, -0.4438,  1.4900],\n        [ 2.0692, -1.0789, -2.0513,  2.0210],\n        [-1.4357, -0.6615, -0.0157,  1.1911]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ab688",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "现在，我们沿列（轴1）拼接矩阵`X`和`H`，沿行（轴0）拼接矩阵`W_xh`和`W_hh`。\n",
    "这两个拼接分别产生形状$(3, 5)$和形状$(5, 4)$的矩阵。\n",
    "再将这两个拼接的矩阵相乘，我们得到与上面相同形状$(3, 4)$的输出矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d88e25a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:47:36.387550Z",
     "iopub.status.busy": "2022-12-07T16:47:36.386740Z",
     "iopub.status.idle": "2022-12-07T16:47:36.395732Z",
     "shell.execute_reply": "2022-12-07T16:47:36.394713Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ],
    "ExecuteTime": {
     "end_time": "2023-09-01T06:01:37.135792Z",
     "start_time": "2023-09-01T06:01:37.131472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.4151, -0.3658, -0.4438,  1.4900],\n        [ 2.0692, -1.0789, -2.0513,  2.0210],\n        [-1.4357, -0.6615, -0.0157,  1.1911]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea490105",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## 基于循环神经网络的字符级语言模型\n",
    "\n",
    "回想一下 :numref:`sec_language_model`中的语言模型，\n",
    "我们的目标是根据过去的和当前的词元预测下一个词元，\n",
    "因此我们将原始序列移位一个词元作为标签。\n",
    "Bengio等人首先提出使用神经网络进行语言建模\n",
    " :cite:`Bengio.Ducharme.Vincent.ea.2003`。\n",
    "接下来，我们看一下如何使用循环神经网络来构建语言模型。\n",
    "设小批量大小为1，批量中的文本序列为“machine”。\n",
    "为了简化后续部分的训练，我们考虑使用\n",
    "*字符级语言模型*（character-level language model），\n",
    "将文本词元化为字符而不是单词。\n",
    " :numref:`fig_rnn_train`演示了\n",
    "如何通过基于字符级语言建模的循环神经网络，\n",
    "使用当前的和先前的字符预测下一个字符。\n",
    "\n",
    "![基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”](../img/rnn-train.svg)\n",
    ":label:`fig_rnn_train`\n",
    "\n",
    "在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，\n",
    "然后利用交叉熵损失计算模型输出和标签之间的误差。\n",
    "由于隐藏层中隐状态的循环计算，\n",
    " :numref:`fig_rnn_train`中的第$3$个时间步的输出$\\mathbf{O}_3$\n",
    "由文本序列“m”“a”和“c”确定。\n",
    "由于训练数据中这个文本序列的下一个字符是“h”，\n",
    "因此第$3$个时间步的损失将取决于下一个字符的概率分布，\n",
    "而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。\n",
    "\n",
    "在实践中，我们使用的批量大小为$n>1$，\n",
    "每个词元都由一个$d$维向量表示。\n",
    "因此，在时间步$t$输入$\\mathbf X_t$将是一个$n\\times d$矩阵，\n",
    "这与我们在 :numref:`subsec_rnn_w_hidden_states`中的讨论相同。\n",
    "\n",
    "## 困惑度（Perplexity）\n",
    ":label:`subsec_perplexity`\n",
    "\n",
    "最后，让我们讨论如何度量语言模型的质量，\n",
    "这将在后续部分中用于评估基于循环神经网络的模型。\n",
    "一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。\n",
    "考虑一下由不同的语言模型给出的对“It is raining ...”（“...下雨了”）的续写：\n",
    "\n",
    "1. \"It is raining outside\"（外面下雨了）；\n",
    "1. \"It is raining banana tree\"（香蕉树下雨了）；\n",
    "1. \"It is raining piouw;kcj pwepoiut\"（piouw;kcj pwepoiut下雨了）。\n",
    "\n",
    "就质量而言，例$1$显然是最合乎情理、在逻辑上最连贯的。\n",
    "虽然这个模型可能没有很准确地反映出后续词的语义，\n",
    "比如，“It is raining in San Francisco”（旧金山下雨了）\n",
    "和“It is raining in winter”（冬天下雨了）\n",
    "可能才是更完美的合理扩展，\n",
    "但该模型已经能够捕捉到跟在后面的是哪类单词。\n",
    "例$2$则要糟糕得多，因为其产生了一个无意义的续写。\n",
    "尽管如此，至少该模型已经学会了如何拼写单词，\n",
    "以及单词之间的某种程度的相关性。\n",
    "最后，例$3$表明了训练不足的模型是无法正确地拟合数据的。\n",
    "\n",
    "我们可以通过计算序列的似然概率来度量模型的质量。\n",
    "然而这是一个难以理解、难以比较的数字。\n",
    "毕竟，较短的序列比较长的序列更有可能出现，\n",
    "因此评估模型产生托尔斯泰的巨著《战争与和平》的可能性\n",
    "不可避免地会比产生圣埃克苏佩里的中篇小说《小王子》可能性要小得多。\n",
    "而缺少的可能性值相当于平均数。\n",
    "\n",
    "在这里，信息论可以派上用场了。\n",
    "我们在引入softmax回归\n",
    "（ :numref:`subsec_info_theory_basics`）时定义了熵、惊异和交叉熵，\n",
    "并在[信息论的在线附录](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)\n",
    "中讨论了更多的信息论知识。\n",
    "如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。\n",
    "一个更好的语言模型应该能让我们更准确地预测下一个词元。\n",
    "因此，它应该允许我们在压缩序列时花费更少的比特。\n",
    "所以我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    ":eqlabel:`eq_avg_ce_for_lm`\n",
    "\n",
    "其中$P$由语言模型给出，\n",
    "$x_t$是在时间步$t$从该序列中观察到的实际词元。\n",
    "这使得不同长度的文档的性能具有了可比性。\n",
    "由于历史原因，自然语言处理的科学家更喜欢使用一个叫做*困惑度*（perplexity）的量。\n",
    "简而言之，它是 :eqref:`eq_avg_ce_for_lm`的指数：\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n",
    "\n",
    "困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。\n",
    "我们看看一些案例。\n",
    "\n",
    "* 在最好的情况下，模型总是完美地估计标签词元的概率为1。\n",
    "  在这种情况下，模型的困惑度为1。\n",
    "* 在最坏的情况下，模型总是预测标签词元的概率为0。\n",
    "  在这种情况下，困惑度是正无穷大。\n",
    "* 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。\n",
    "  在这种情况下，困惑度等于词表中唯一词元的数量。\n",
    "  事实上，如果我们在没有任何压缩的情况下存储序列，\n",
    "  这将是我们能做的最好的编码方式。\n",
    "  因此，这种方式提供了一个重要的上限，\n",
    "  而任何实际模型都必须超越这个上限。\n",
    "\n",
    "在接下来的小节中，我们将基于循环神经网络实现字符级语言模型，\n",
    "并使用困惑度来评估这样的模型。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 对隐状态使用循环计算的神经网络称为循环神经网络（RNN）。\n",
    "* 循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。\n",
    "* 循环神经网络模型的参数数量不会随着时间步的增加而增加。\n",
    "* 我们可以使用循环神经网络创建字符级语言模型。\n",
    "* 我们可以使用困惑度来评价语言模型的质量。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果我们使用循环神经网络来预测文本序列中的下一个字符，那么任意输出所需的维度是多少？\n",
    "1. 为什么循环神经网络可以基于文本序列中所有先前的词元，在某个时间步表示当前词元的条件概率？\n",
    "1. 如果基于一个长序列进行反向传播，梯度会发生什么状况？\n",
    "1. 与本节中描述的语言模型相关的问题有哪些？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac362b",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/2100)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
