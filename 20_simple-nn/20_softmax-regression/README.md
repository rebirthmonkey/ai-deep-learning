# softmax回归

## 简介

Softmax 回归用于多分类问题，它的输出给每个分类一个“置信度”概率，而所有分类“置信度”概率总和为 1。

一个简单的例子，假设每次输入是一个 $2\times2$ 的灰度图像。可以用一个标量表示每个像素值，每个图像对应四个特征 $x_1, x_2, x_3, x_4$。此外，假设每个图像属于类别“猫”“鸡”和“狗”中的一个。希望模型的输出 $\hat{y}_j$ 可以视为属于类 $j$ 的概率（置信度），然后选择具有最大输出值的类别 $\operatorname*{argmax}_j y_j$ 作为最终的预测。例如，如果 $\hat{y}_1$、$\hat{y}_2$ 和 $\hat{y}_3$ 分别为 0.1、0.8 和 0.1，那么预测的类别是 2，在例子中代表“鸡”。

## One-hot Encoding

在多分类问题中，因为不同类别之间的自然顺序其实没有关联，所以需要采用 One-hot Encoding 将不同类别独立出来看。独热编码是一个向量，它的分量和类别一样多。类别对应的分量设置为 1，其他所有分量设置为 0。One-hot 是一种用于将离散的词汇表示成二进制向量的方法，每个词汇对应一个唯一的向量，其中只有一个元素为 1，其余元素为 0。这个 1 位通常表示词汇在词汇表中的位置。one-hot 的步骤如下：

- 构建词汇表：首先，需要构建一个包含文本数据中所有不同词汇的词汇表。每个词汇都被赋予一个唯一的整数标识，通常按照它们在词汇表中的顺序分配。
- one-hot 编码：对于每个词汇，将对应的整数标识转化为一个二进制向量，其中只有一个位置为 1，而其他位置都为 0。这个 1 位的位置表示词汇的标识。

举个例子，假设我们有一个包含 4 个词汇的词汇表：["apple", "banana", "cherry", "date"]，那么独热编码后的向量如下：

```json
"apple"：[1, 0, 0, 0]
"banana"：[0, 1, 0, 0]
"cherry"：[0, 0, 1, 0]
"date"：[0, 0, 0, 1]
```

通用数学公式：v("x")∈R^N，one-hot 的优点是简单易懂，每个词汇都有唯一的编码。然而，它的缺点是无法捕捉单词间的相似性和语义关系，每个词都是独立的。除此之外，向量维度与词汇表大小相关，要为每个词创建一个维度，也导致了对于大型词汇表其向量会十分稀疏，只有一个位置为 1 其余都为 0。

## 学习算法

为了估计所有可能类别的条件概率，需要一个有多个输出的模型，每个类别对应一个输出。为了解决线性模型的分类问题，需要和输出一样多的**仿射函数（affine function）**，每个输出对应于它自己的仿射函数。

在例子中，由于有 4 个特征和 3 个可能的输出类别，将需要 12 个标量来表示权重（带下标的 $w$），3 个标量来表示偏置（带下标的 $b$）。为每个输入计算 3 个未规范化的预测：$o_1$、$o_2$ 和 $o_3$。

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$
为了更简洁地表达模型，仍然使用线性代数符号，通过向量形式表达为 $\mathbf{O} = \mathbf{W} \mathbf{X} + \mathbf{B}$。由此，将所有权重放到一个 $3 \times 4$ 矩阵中。对于给定数据样本的特征 $\mathbf{X}$，输出是由权重与输入特征进行矩阵-向量乘法再加上偏置 $\mathbf{B}$ 得到的。

### 网络结构

由于计算每个输出 $o_1$、$o_2$ 和 $o_3$ 取决于所有输入 $x_1$、$x_2$、$x_3$ 和 $x_4$，所以 softmax 回归的输出层也是全连接层。

![softmaxreg](figures/softmaxreg.svg)

#### 激活函数：Softmax()

softmax() 通常被视为激活函数，特别是在用于多类别分类问题的神经网络的输出层。softmax() 可以将任何一组实数映射为一组在 0 和 1 之间的实数，而且这组实数的总和为 1，因此常常被用来表示概率分布。

但值得注意的是，softmax() 与很多其他常用的激活函数（如 ReLU、sigmoid 等）有一些不同之处。ReLU 和 sigmoid 函数都是对每个元素逐一作用的元素级函数，而 softmax() 则是对一组元素作为整体来处理的，这是因为它的输出取决于输入元素的相对大小。也就是说，softmax() 会考虑所有输入元素的值，并将其归一化为一个概率分布。这特性使得 softmax() 非常适合用于表示一组元素的相对比例或者概率，这在多类别分类问题的输出层尤其有用。

#### 表达式

softmax() 函数能够将未规范化的预测变换为非负数并且总和为 1，同时让模型保持可导的性质。首先对每个未规范化的预测求幂，这样可以确保输出非负。为了确保最终输出的概率值总和为 1，再让每个求幂后的结果除以它们的总和。如下式：

$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$，对所有的 $j$ 总有 $0 \leq \hat{y}_j \leq 1$。因此，$\hat{\mathbf{y}}$ 可以视为一个正确的概率分布。

Softmax 运算不会改变未规范化的预测 $\mathbf{o}$ 之间的大小次序，只会确定分配给每个类别的概率。因此，在预测过程中，仍然可以用下式来选择最有可能的类别。

### 度量

#### 代价函数：Cross-Entropy Loss

当用 softmax() 作为激活函数时，对应的损失函数一般是 Cross-Entropy 对数似然函数 $𝐽(𝑊,𝑏,𝑎^𝐿,𝑦)=−∑_𝑘𝑦_𝑘𝑙𝑛𝑎^𝐿_𝑘$，其中 $𝑦_𝑘$ 的取值为 0 或 1。如果某一训练样本的输出为第 i 类，则 $𝑦_𝑖=1$，其余的 $𝑗≠𝑖$ 都有 $𝑦_𝑗=0$。由于每个样本只属于一个类别，所以这个对数似然函数可以简化为：$𝐽(𝑊,𝑏,𝑎^𝐿,𝑦)=−𝑙𝑛𝑎^𝐿_𝑖$，其中 𝑖 即为训练样本真实的类别序号。

可见代价函数只和真实类别对应的输出有关，这样假设真实类别是第 i 类，对于真实类别第 i 类，他对应的第 j 个 w 链接 $𝑤^𝐿_{𝑖𝑗}$ 对应的梯度计算为：$\frac{∂𝐽(𝑊,𝑏,𝑎^𝐿,𝑦)}{∂𝑤^𝐿_{𝑖𝑗}}=(𝑎^𝐿_𝑖−1)𝑎^{𝐿−1}_𝑗$。同样的可以得到 $𝑏^𝐿_𝑖$ 的梯度表达式为：$\frac{∂𝐽(𝑊,𝑏,𝑎^𝐿,𝑦)}{∂𝑏^𝐿_𝑖}=𝑎^𝐿_𝑖−1$。

可见，梯度计算也很简洁，也没有之前的训练速度慢的问题。举个例子，假如对于第 2 类的训练样本，通过前向算法计算的未激活输出为 (1,5,3)，则得到 softmax 激活后的概率输出为 (0.015,0.866,0.117)。由于类别是第二类，则反向传播的梯度应该为：(0.015,0.866-1,0.117)。

#### 整体度量：Accuracy

代价函数用于在训练时度量，而 Accuracy 用于展示整个数据集的效果，等于正确预测数与预测总数的比例。它不是用来做优化，只是用来更直观地评估模型的整体性能。

### 优化器

#### 批量梯度下降法



